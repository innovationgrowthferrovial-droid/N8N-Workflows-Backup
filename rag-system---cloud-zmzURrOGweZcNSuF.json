{"createdAt":"2025-08-15T11:40:53.140Z","updatedAt":"2025-08-15T11:53:02.930Z","id":"zmzURrOGweZcNSuF","name":"RAG System - Cloud","active":false,"isArchived":false,"nodes":[{"parameters":{},"id":"51790737-794a-43a4-b26e-ee6d6a85b4c0","name":"When clicking ‘Test workflow’","type":"n8n-nodes-base.manualTrigger","typeVersion":1,"position":[-240,352]},{"parameters":{"operation":"pdf","options":{}},"id":"612b4c3a-c248-43c8-919e-9a445a632c9d","name":"Extract from File","type":"n8n-nodes-base.extractFromFile","typeVersion":1,"position":[336,384]},{"parameters":{"mode":"runOnceForEachItem","jsCode":"const chunks = [];\nconst chunkSize = 1000;\nconst chunkOverlap = 200;\nconst text = $('Extract from File').item.json.text.replace(/\\n/, '');\n\nfor (let i=0,j=Math.round(text.length/chunkSize)+1;i<j;i++) {\n  chunks.push(\n    text.substr(\n      Math.max(0,(i * chunkSize)-chunkOverlap),\n      chunkSize\n    )\n  );\n}\n\nreturn { chunks };"},"id":"1777c252-7491-439f-87dc-3dea189ff911","name":"Create Chunks From Doc","type":"n8n-nodes-base.code","typeVersion":2,"position":[1120,464]},{"parameters":{"jsonMode":"expressionData","jsonData":"={{\n{\n  \"content\": `${$json.text }\\n---\\n${$json.chunk}`\n}\n}}","options":{"metadata":{"metadataValues":[{"name":"title","value":"={{ $json.title }}"}]}}},"id":"a61f2c2c-1be3-4d19-8e16-db03317d9cb0","name":"Default Data Loader","type":"@n8n/n8n-nodes-langchain.documentDefaultDataLoader","typeVersion":1,"position":[2864,672]},{"parameters":{"chunkSize":2000,"options":{}},"id":"05901998-a5b9-437e-986f-d2ac808dc019","name":"Recursive Character Text Splitter","type":"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter","typeVersion":1,"position":[2864,784]},{"parameters":{"options":{}},"id":"a195e800-62a9-4abb-aa92-0d69f0049a1b","name":"When chat message received","type":"@n8n/n8n-nodes-langchain.chatTrigger","typeVersion":1.1,"position":[208,1296],"webhookId":"436ca65c-46ca-4f8c-86e2-b8633b428eea"},{"parameters":{"options":{}},"id":"8119c4cc-daf9-479b-bb06-3feb5978c9c4","name":"Anthropic Chat Model1","type":"@n8n/n8n-nodes-langchain.lmChatAnthropic","typeVersion":1.2,"position":[224,1536]},{"parameters":{},"id":"52ec32a7-a630-4237-88d0-c04066551c1e","name":"Window Buffer Memory","type":"@n8n/n8n-nodes-langchain.memoryBufferWindow","typeVersion":1.2,"position":[528,1472]},{"parameters":{"content":"## 2. Split Document Into Chunks\nUnlike traditional vector store workflows, we want to split our document prior to embedding and this is easily achieved using the Code node. Feel free to adjust the text splitting params or replace it entirely to suit the needs of your data.\n\nYou may need to play around and adjust the chunksize for your particular data. Contextual retrieval as described in the article is expected to return 20 results so best to keep these small.","height":513.3089035768523,"width":553.1909664515983,"color":7},"id":"2474d01e-59c1-4509-8845-5890a12fe8e3","name":"Sticky Note","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[896,208]},{"parameters":{"content":"## 3. Generate Sparse Vector and Contextual Text For Chunk\nWith our chunks, we'll want to achieve the following:\n(1) **Generate a contextual summary of what the chunk is about relative to the whole document**.\nFor this, we'll use the basic LLM node using Antrophic's Claude Haiku model with the recommended prompt as shared in the article.\n(2) **Generate a sparse vector for the chunk and summary**\nWe can use the python code node to generate TF-IDF sparse vectors with the scikit-learn library. Good to know, this library doesn't require external dependency setup steps and auto-installs on first time use.\n\nOnce we have our generated values, we'll combine them with the chunk object using the Edit Fields node.","height":748.1255853485875,"width":1019.742667955435,"color":7},"id":"30e1e67c-5331-4c05-a262-7523bb235663","name":"Sticky Note1","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1488,208]},{"parameters":{"content":"## 4. Insert Docs to Qdrant (via Langchain Code Node)\nUnfortunately, n8n (or rather langchain) doesn't support inserting sparse vectors so we'll have to build our own \"Insert Documents\" node using a Langchain Code Node. In this Langchain code node, we'll forego the langchain vectorstore node and use the Qdrant client SDK directly instead.\n\n**Note** To avoid duplication, this node will also delete existing vectors by document title. It does so by tagging each vector with the document title we extracted earlier then when run again, performs a Qdrant delete by filter before upserting.\n\n**Required:**\nTo use this demonstration, you must complete the following:\n* Ensure your Qdrant instance is running and set the URL in the node\n* Create the Qdrant collection as instructed (see yellow sticky)\n","height":783.6896392386983,"width":828.7526122872351,"color":7},"id":"a0e5d7b6-0244-473e-b418-e999c88d68ed","name":"Sticky Note2","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[2544,176]},{"parameters":{"content":"## 5. Retrieval using Sparse Vectors and ReRanker (Chat Agent Example)\nFor retrieval, we want to be able to (1) query with both dense and sparse vectors and (2) apply a rerank algorithm to our vector store docs. We can setup a custom vector store tool which does both using a custom Langchain Code node.\n\n**Required:**\nTo use this demonstration, you must complete the following:\n* Installed the updated version of the @Qdrant/js-client-rest module\n* Ensure your Qdrant instance is running and set the URL in the \"Qdrant with Cohere ReRank\" subnode\n* Add your Cohere API key in the \"Qdrant with Cohere ReRank\" subnode.","height":828.8619472986827,"width":973.8052093023243,"color":7},"id":"787d84c2-9c38-4101-ab3f-b8c0c4a9ac02","name":"Sticky Note3","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[64,992]},{"parameters":{},"id":"568e6c57-8b73-4d45-ab45-240c1427de4e","name":"Execute Workflow Trigger","type":"n8n-nodes-base.executeWorkflowTrigger","typeVersion":1,"position":[1120,1808]},{"parameters":{"name":"get_sparse_vector","description":"Generates TD-IDF sparse vector for query","workflowId":{"__rl":true,"value":"={{ $workflow.id }}","mode":"id"},"fields":{"values":[{"name":"route","stringValue":"get_sparse_vectors"}]}},"id":"925ce90b-4f7f-46c3-a2ab-f4fa1c7d87c0","name":"Get Sparse Vector Tool","type":"@n8n/n8n-nodes-langchain.toolWorkflow","typeVersion":1.2,"position":[800,1600]},{"parameters":{"assignments":{"assignments":[{"id":"87bc3071-4179-4aed-aa88-37c6381d8b73","name":"query","value":"Who created Bitcoin?","type":"string"}]},"options":{}},"id":"bfbe0c69-4b40-4506-aa04-120e061d620a","name":"Query","type":"n8n-nodes-base.set","typeVersion":3.4,"position":[1248,1264]},{"parameters":{"name":"get_sparse_vector","description":"Generates TD-IDF sparse vector for query","workflowId":{"__rl":true,"value":"={{ $workflow.id }}","mode":"id"}},"id":"3d013432-1c44-4e85-90d7-6f52e51ccb88","name":"Get Sparse Vector Tool1","type":"@n8n/n8n-nodes-langchain.toolWorkflow","typeVersion":1.2,"position":[1616,1440]},{"parameters":{"content":"## 6. Retrieval using Sparse Vectors and ReRanker (Retrieval Example)\nThis demonstration is similar to the previous step but is not using an AI Agent.\n\n**Required:**\nTo use this demonstration, you must complete the following:\n* Installed the updated version of the @Qdrant/js-client-rest module\n* Ensure your Qdrant instance is running and set the URL in the \"Qdrant with Cohere ReRank1\" node\n* Add your Cohere API key in the \"Qdrant with Cohere ReRank1\" node.","height":683.3722976015338,"width":838.4124151865863,"color":7},"id":"f9acdef6-5096-4faf-8fd1-ed021a8c3d0b","name":"Sticky Note4","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1056,992]},{"parameters":{"model":"claude-3-haiku-20240307","options":{}},"id":"a6b9fab9-ba03-4551-ba7b-fe8961c48599","name":"Anthropic Chat Model2","type":"@n8n/n8n-nodes-langchain.lmChatAnthropic","typeVersion":1.2,"position":[1648,784]},{"parameters":{"content":"### Create Collection!\nYou need to create a Qdrant Collection as follows:\n\n* Go to http[s]:\\//<qdrant_url>/dashboard#/console\nIf you are hosting locally, this is usually http://localhost:6333/dashboard#/console\n* Copy the following into the left panel. This will tell Qdrant to create a new collection called “contextual_retrieval_example”. You can change this of course but you’ll also need to change all “collectionName” references in the template as well!\n\n```\nPUT collections/contextual_retrieval_example\n{\n  \"vectors\": {\n    \"default\": {\n      \"distance\": \"Cosine\",\n      \"size\": 1024\n    }\n  },\n  \"sparse_vectors\": {\n    \"bm42\": {\n      \"modifier\": \"idf\"\n    }\n  }\n}\n```","height":505.701259707935,"width":516.3129732020735},"id":"9f0ce8f0-2fa5-4dde-9c06-7fff67442aa3","name":"Sticky Note5","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[3264,512]},{"parameters":{"fieldToSplitOut":"chunks","options":{"destinationFieldName":"chunk"}},"id":"e7439d9f-1c80-4d1f-a0af-7b066c8140c7","name":"Chunks To List","type":"n8n-nodes-base.splitOut","typeVersion":1,"position":[1600,528]},{"parameters":{"mode":"runOnceForEachItem","language":"python","pythonCode":"texts = [f\"{_('Generate Contextual Text').item.json.text}\\n---\\n{_('Chunks To List').item.json.chunk}\"]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the texts to generate TF-IDF vectors\nX = vectorizer.fit_transform(texts)\n\nreturn {\n  \"sparse\": {\n    \"indices\": X.indices.tolist(),\n    \"values\": X.data.tolist()\n  }\n}"},"id":"67bbe5fb-1361-4afb-bbe0-1765ba3c0a76","name":"Generate TF-IDF Sparse Vectors","type":"n8n-nodes-base.code","typeVersion":2,"position":[2128,528]},{"parameters":{"assignments":{"assignments":[{"id":"069d067c-3534-4939-8ff4-34dee02a9436","name":"chunk","value":"={{ $('Chunks To List').item.json.chunk }}","type":"string"},{"id":"24e01f4f-e156-47e9-a89e-9cbdccda6bd4","name":"text","value":"={{ $('Generate Contextual Text').item.json.text }}","type":"string"},{"id":"fa48ddaa-4658-463a-b1af-8308c24e325a","name":"sparse","value":"={{ $json.sparse }}","type":"object"},{"id":"442efe87-a826-402c-aadc-909923915d30","name":"title","value":"={{ $('Get Doc Attributes').first().json.output.title }}","type":"string"}]},"options":{}},"id":"9c8f4e9d-02d1-422e-975f-ab15aebffde4","name":"Get Values","type":"n8n-nodes-base.set","typeVersion":3.4,"position":[2304,528]},{"parameters":{"mode":"runOnceForEachItem","language":"python","pythonCode":"import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the texts to generate TF-IDF vectors\ntexts = [_input.item.json.query]\nX = vectorizer.fit_transform(texts)\n\nreturn {\n  \"response\": {\n    \"indices\": X.indices.tolist(),\n    \"values\": X.data.tolist()\n  }\n}"},"id":"9932204f-d33c-43e2-8eb2-deb733644c84","name":"Generate Sparse Vectors","type":"n8n-nodes-base.code","typeVersion":2,"position":[1520,1808]},{"parameters":{"content":"## Implementing Antrophic's Contextual Retrieval \n\nThis workflow is inspired by the Antrophic article, [\"Introducing Contextual Retrieval\"](https://www.anthropic.com/news/contextual-retrieval), and tries to replicate the instructions given.\n\n**Original post here**: https://community.n8n.io/t/building-the-ultimate-rag-setup-with-contextual-summaries-sparse-vectors-and-reranking/54861/1\n\n### Changelog\n2024-10-10 Updated to 1.62.1\n\n### Need Help?\nJoin the [Discord](https://discord.com/invite/XPKeKXeB7d) or ask in the [Forum](https://community.n8n.io/)!\n","height":634.9891793542789,"width":405.66570496276904},"id":"ca071b5a-05ca-4804-bba9-cd4cf63d2305","name":"Sticky Note6","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[-384,-80]},{"parameters":{"modelName":"embed-english-v3.0"},"id":"1ee99e55-f152-4cae-8622-78e46ba9379e","name":"Embeddings Cohere","type":"@n8n/n8n-nodes-langchain.embeddingsCohere","typeVersion":1,"position":[2720,672],"credentials":{"cohereApi":{"id":"PRiHAa0S7ALOajrr","name":"CohereApi account"}}},{"parameters":{"modelName":"embed-english-v3.0"},"id":"f7e7a707-0b3c-4b5c-803c-19bde3f6d6df","name":"Embeddings Cohere1","type":"@n8n/n8n-nodes-langchain.embeddingsCohere","typeVersion":1,"position":[640,1616],"credentials":{"cohereApi":{"id":"PRiHAa0S7ALOajrr","name":"CohereApi account"}}},{"parameters":{"modelName":"embed-english-v3.0"},"id":"9d253d43-3450-4857-8da3-a6ada45cc633","name":"Embeddings Cohere2","type":"@n8n/n8n-nodes-langchain.embeddingsCohere","typeVersion":1,"position":[1424,1440],"credentials":{"cohereApi":{"id":"PRiHAa0S7ALOajrr","name":"CohereApi account"}}},{"parameters":{"content":"### PART 1 of 2\nThis part generates and inserts into the vector store. You only have to do this once per document.","height":102.577757187954,"width":389.2978313113204,"color":5},"id":"e0ead510-c482-4dec-9e95-ed7f1a0a41bf","name":"Sticky Note8","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[64,80]},{"parameters":{"content":"### PART 2 of 2\nThis part demostrates 2 examples of retrieve or query your sparse vectors.","height":80,"width":524.5591143796955,"color":5},"id":"ea5b7a88-c16f-44dc-9832-66919db2b81b","name":"Sticky Note9","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[64,880]},{"parameters":{"rules":{"values":[{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"strict","version":2},"conditions":[{"leftValue":"={{ $json.route }}","rightValue":"get_sparse_vectors","operator":{"type":"string","operation":"equals"}}],"combinator":"and"},"renameOutput":true,"outputKey":"get sparse vectors"},{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"strict","version":2},"conditions":[{"id":"84ac9b84-0e46-45da-b719-843d947ea429","leftValue":"={{ $json.route }}","rightValue":"my_other_thing","operator":{"type":"string","operation":"equals","name":"filter.operator.equals"}}],"combinator":"and"},"renameOutput":true,"outputKey":"my other thing"}]},"options":{"fallbackOutput":"none"}},"id":"fb73d54c-2d31-41b6-95d7-924a9d3b2116","name":"Router","type":"n8n-nodes-base.switch","typeVersion":3.2,"position":[1280,1808]},{"parameters":{"promptType":"define","text":"=<document> \n{{ $('Extract from File').first().json.text }} \n</document>\nHere is the chunk we want to situate within the whole document \n<chunk> \n{{ $json.chunk }}\n</chunk> \nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. "},"id":"c8b064c4-a44a-4219-a48c-b9b024ed3da9","name":"Generate Contextual Text","type":"@n8n/n8n-nodes-langchain.chainLlm","typeVersion":1.4,"position":[1776,528]},{"parameters":{"options":{}},"id":"e37220b3-2852-4f53-8ebb-20eb0e7533a8","name":"AI Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":1.6,"position":[400,1296]},{"parameters":{"content":"### Sparse Vector Tool for Agent\nUnfortunately there is a bug linking code tool to custom langchain code node so this is the only approach until that is fixed!","height":287.1680736478712,"width":652.0156501726113,"color":6},"id":"9db14712-33ca-46ff-8e69-fceb785eca05","name":"Sticky Note7","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1072,1696]},{"parameters":{"url":"https://www.jaai.net/vol1/JAAI-V1N2-6.pdf","options":{}},"id":"d34341df-5fe1-4b91-8eb7-ff6396a27c1d","name":"Get Document","type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[160,384]},{"parameters":{"model":"claude-3-haiku-20240307","options":{}},"id":"2352dff7-6a98-49ba-9dc2-f4ac3fa86f46","name":"Anthropic Chat Model","type":"@n8n/n8n-nodes-langchain.lmChatAnthropic","typeVersion":1.2,"position":[480,592]},{"parameters":{"text":"={{ $json.text }}","attributes":{"attributes":[{"name":"title","description":"The title of the document.","required":true}]},"options":{}},"id":"a015a990-f25d-4a41-aa29-a6a05fe0cf36","name":"Get Doc Attributes","type":"@n8n/n8n-nodes-langchain.informationExtractor","typeVersion":1,"position":[528,384]},{"parameters":{"content":"## 1. Import Document PDF\n\nFor this demonstration, we'll use the Bitcoin Whitepaper from bitcoin.org.\nWe'll take this time to also extract specific document attributes which we can use as metadata.","height":513.3089035768523,"width":807.2147979360316,"color":7},"id":"f6545020-7f62-42cb-8533-c43a1b0bda37","name":"Sticky Note10","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[64,208]},{"parameters":{"code":{"execute":{"code":"const { randomUUID } = require('crypto') // Enable the crypto lib using env var NODE_FUNCTION_ALLOW_BUILTIN=crypto\nconst { QdrantClient } = require('@qdrant/js-client-rest');\n\n// 1. Qdrant config\nconst client = new QdrantClient({ url: 'http://qdrant:6333' });\nconst collectionName = 'contextual_retrieval_example';\n\n// 2. Inputs\nconst inputData = this.getInputData();\nconst embeddings = await this.getInputConnectionData('ai_embedding', 0);\nconst documentLoader = await this.getInputConnectionData('ai_document', 0);\n\n// 3. Run document loader\nconst docs = await documentLoader.processAll(inputData);\n\n// 4. generate points with sparse vectors\nconst points = [];\nlet vector = {};\nfor (let i=0,j=docs.length;i<j;i++) {\n  points.push({\n    id: randomUUID(),\n    vector: {\n      default: await embeddings.embedQuery(docs[i].pageContent),\n      bm42: inputData[i].json.sparse,\n    },\n    payload: {\n      content: docs[i].pageContent,\n      metadata: docs[i].metadata,\n    }\n  })\n}\n\n// 5. Delete existing vectors by title\nawait client.delete(collectionName, {\n  filter: {\n    must: [\n      {\n        key: \"metadata.title\",\n        match: { \"value\": docs[0].metadata.title }\n      }\n    ]\n  }\n});\n\n// 6. Upsert into Qdrant\nconst res = await client.upsert(collectionName, { points });\n\nreturn res;"}},"inputs":{"input":[{"type":"main","maxConnections":1,"required":true},{"type":"ai_embedding","maxConnections":1,"required":true},{"type":"ai_document","maxConnections":1,"required":true}]},"outputs":{"output":[{"type":"main"}]}},"id":"41fa4a8d-7b1d-4fcf-9ff0-07ac52f452e9","name":"Insert Documents with Sparse Vectors","type":"@n8n/n8n-nodes-langchain.code","typeVersion":1,"position":[2768,480]},{"parameters":{"code":{"supplyData":{"code":"const { QdrantClient } = require('@qdrant/js-client-rest');\nconst { CohereRerank } = require(\"@langchain/cohere\");\nconst { DynamicTool } = require(\"@langchain/core/tools\");\n\n// 1. Tool Config\nconst name = 'bitcoin_whitepaper_tool';\nconst description = 'Call this tool to get information and/or context from the Bitcoin Whitepaper';\n\n// 2. Qdrant config\nconst client = new QdrantClient({ url: 'http://qdrant:6333' });\nconst collectionName = 'contextual_retrieval_example';\nconst limit = 20;\n\n// 3. Cohere config\nconst cohereRerank = new CohereRerank({\n  apiKey: '<MY_COHERE_API_KEY>', // Default\n  model: \"rerank-english-v3.0\", // Default\n});\n\n// 4. Inputs\nconst embeddings = await this.getInputConnectionData('ai_embedding', 0);\nconst sparseVectorTool = await this.getInputConnectionData('ai_tool', 0);\n\n// 5. Tool definition\nconst vectorStoreTool = new DynamicTool({\n  name,\n  description,\n  func: async (input) => {\n    const denseVector = await embeddings.embedQuery(input);\n    const sparseVector = JSON.parse(await sparseVectorTool.invoke(input));\n\n    const response = await client.query(collectionName, {\n      prefetch: [\n        {\n          query: denseVector,\n          using: 'default',\n          limit: 100\n        },\n        {\n          query: sparseVector,\n          using: 'bm42',\n          limit: 100\n        }\n     ],\n     query: { fusion: 'rrf' },\n     with_payload: true,\n     limit,\n    });\n    \n    const docs = response.points.map(res => ({\n      pageContent: res.payload.content,\n      metadata: res.payload.metadata\n    }));\n    const rankings = await cohereRerank.rerank(docs, input);\n    rankings.forEach(rank => { docs[rank.index].score = rank.relevanceScore });\n\n    const rankedDocs = docs.toSorted((a,b) => b.score-a.score);\n    return JSON.stringify(rankedDocs);\n  }\n});\n\nreturn vectorStoreTool;"}},"inputs":{"input":[{"type":"ai_embedding","maxConnections":1,"required":true},{"type":"ai_tool","maxConnections":1,"required":true}]},"outputs":{"output":[{"type":"ai_tool"}]}},"id":"84746bd6-0ad7-4d3a-b770-86de6b3592f7","name":"Qdrant with Cohere ReRank","type":"@n8n/n8n-nodes-langchain.code","typeVersion":1,"position":[640,1472]},{"parameters":{"code":{"execute":{"code":"const { QdrantClient } = require('@qdrant/js-client-rest');\nconst { CohereRerank } = require(\"@langchain/cohere\");\nconst { DynamicTool } = require(\"@langchain/core/tools\");\n\n// 1. Tool Config\nconst name = 'bitcoin_whitepaper_tool';\nconst description = 'Call this tool to get information and/or context from the Bitcoin Whitepaper';\n\n// 2. Qdrant config\nconst client = new QdrantClient({ url: 'http://qdrant:6333' });\nconst collectionName = 'contextual_retrieval_example';\nconst limit = 20;\n\n// 3. Cohere config\nconst cohereRerank = new CohereRerank({\n  apiKey: '<MY_COHERE_API_KEY>', // Default\n  model: \"rerank-english-v3.0\", // Default\n});\n\n// 4. Inputs\nconst inputData = await this.getInputData();\nconst embeddings = await this.getInputConnectionData('ai_embedding', 0);\nconst sparseVectorTool = await this.getInputConnectionData('ai_tool', 0);\n\n// 5. Execute\nconst query = inputData[0].json.query;\n\nconst denseVector = await embeddings.embedQuery(query);\nconst sparseVector = JSON.parse(await sparseVectorTool.invoke(query));\n\nconst response = await client.query(collectionName, {\n  prefetch: [\n    {\n      query: denseVector,\n      using: 'default',\n      limit: 100\n    },\n    {\n      query: sparseVector,\n      using: 'bm42',\n      limit: 100\n    }\n ],\n query: { fusion: 'rrf' },\n with_payload: true,\n limit,\n});\n\nconst docs = response.points.map(res => ({\n  pageContent: res.payload.content,\n  metadata: res.payload.metadata\n}));\nconst rankings = await cohereRerank.rerank(docs, query);\nrankings.forEach(rank => { docs[rank.index].score = rank.relevanceScore });\n\nconst rankedDocs = docs.toSorted((a,b) => b.score-a.score);\nreturn rankedDocs;"}},"inputs":{"input":[{"type":"main","maxConnections":1,"required":true},{"type":"ai_embedding","maxConnections":1,"required":true},{"type":"ai_tool","maxConnections":1,"required":true}]},"outputs":{"output":[{"type":"main"}]}},"id":"3c03a7e0-3d15-4d01-9aa2-5670b6ff2440","name":"Qdrant with Cohere ReRank1","type":"@n8n/n8n-nodes-langchain.code","typeVersion":1,"position":[1424,1264]},{"parameters":{"model":{"__rl":true,"mode":"list","value":"gpt-4.1-mini"},"options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatOpenAi","typeVersion":1.2,"position":[624,592],"id":"706d301c-6ffd-4b2f-a3ef-521c0cf868b5","name":"OpenAI Chat Model","credentials":{"openAiApi":{"id":"Nalia168hExYSGD3","name":"OpenAi account"}}},{"parameters":{"model":{"__rl":true,"mode":"list","value":"gpt-4.1-mini"},"options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatOpenAi","typeVersion":1.2,"position":[1824,784],"id":"e06c83d0-0741-4ba0-88fb-d1b0b52d0687","name":"OpenAI Chat Model1","credentials":{"openAiApi":{"id":"Nalia168hExYSGD3","name":"OpenAi account"}}},{"parameters":{"model":{"__rl":true,"mode":"list","value":"gpt-4.1-mini"},"options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatOpenAi","typeVersion":1.2,"position":[384,1472],"id":"3884b83d-653a-4c02-a743-02810ff7120b","name":"OpenAI Chat Model2","credentials":{"openAiApi":{"id":"Nalia168hExYSGD3","name":"OpenAi account"}}}],"connections":{"When clicking ‘Test workflow’":{"main":[[{"node":"Get Document","type":"main","index":0}]]},"Extract from File":{"main":[[{"node":"Get Doc Attributes","type":"main","index":0}]]},"Recursive Character Text Splitter":{"ai_textSplitter":[[{"node":"Default Data Loader","type":"ai_textSplitter","index":0}]]},"When chat message received":{"main":[[{"node":"AI Agent","type":"main","index":0}]]},"Anthropic Chat Model1":{"ai_languageModel":[[]]},"Window Buffer Memory":{"ai_memory":[[{"node":"AI Agent","type":"ai_memory","index":0}]]},"Default Data Loader":{"ai_document":[[{"node":"Insert Documents with Sparse Vectors","type":"ai_document","index":0}]]},"Execute Workflow Trigger":{"main":[[{"node":"Router","type":"main","index":0}]]},"Get Sparse Vector Tool":{"ai_tool":[[{"node":"Qdrant with Cohere ReRank","type":"ai_tool","index":0}]]},"Query":{"main":[[{"node":"Qdrant with Cohere ReRank1","type":"main","index":0}]]},"Anthropic Chat Model2":{"ai_languageModel":[[]]},"Chunks To List":{"main":[[{"node":"Generate Contextual Text","type":"main","index":0}]]},"Generate TF-IDF Sparse Vectors":{"main":[[{"node":"Get Values","type":"main","index":0}]]},"Get Values":{"main":[[{"node":"Insert Documents with Sparse Vectors","type":"main","index":0}]]},"Get Sparse Vector Tool1":{"ai_tool":[[{"node":"Qdrant with Cohere ReRank1","type":"ai_tool","index":0}]]},"Embeddings Cohere":{"ai_embedding":[[{"node":"Insert Documents with Sparse Vectors","type":"ai_embedding","index":0}]]},"Embeddings Cohere1":{"ai_embedding":[[{"node":"Qdrant with Cohere ReRank","type":"ai_embedding","index":0}]]},"Embeddings Cohere2":{"ai_embedding":[[{"node":"Qdrant with Cohere ReRank1","type":"ai_embedding","index":0}]]},"Create Chunks From Doc":{"main":[[{"node":"Chunks To List","type":"main","index":0}]]},"Router":{"main":[[{"node":"Generate Sparse Vectors","type":"main","index":0}]]},"Generate Contextual Text":{"main":[[{"node":"Generate TF-IDF Sparse Vectors","type":"main","index":0}]]},"Get Document":{"main":[[{"node":"Extract from File","type":"main","index":0}]]},"Anthropic Chat Model":{"ai_languageModel":[[]]},"Get Doc Attributes":{"main":[[{"node":"Create Chunks From Doc","type":"main","index":0}]]},"Qdrant with Cohere ReRank":{"ai_tool":[[{"node":"AI Agent","type":"ai_tool","index":0}]]},"OpenAI Chat Model":{"ai_languageModel":[[{"node":"Get Doc Attributes","type":"ai_languageModel","index":0}]]},"OpenAI Chat Model1":{"ai_languageModel":[[{"node":"Generate Contextual Text","type":"ai_languageModel","index":0}]]},"OpenAI Chat Model2":{"ai_languageModel":[[{"node":"AI Agent","type":"ai_languageModel","index":0}]]}},"settings":{"executionOrder":"v1"},"staticData":null,"meta":null,"pinData":{},"versionId":"34ec97ca-45be-4d7e-a682-7443e9b79d19","triggerCount":0,"shared":[{"createdAt":"2025-08-15T11:40:53.140Z","updatedAt":"2025-08-15T11:40:53.140Z","role":"workflow:owner","workflowId":"zmzURrOGweZcNSuF","projectId":"8XYD7Ip26tz9IjW7"}],"tags":[]}