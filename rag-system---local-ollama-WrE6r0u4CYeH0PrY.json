{"createdAt":"2025-08-15T11:37:21.391Z","updatedAt":"2025-08-15T11:38:50.661Z","id":"WrE6r0u4CYeH0PrY","name":"RAG System - Local Ollama","active":false,"isArchived":false,"nodes":[{"parameters":{},"id":"c105f1ce-72b6-4538-9e26-1735ee1b47ed","name":"When clicking ‘Test workflow’","type":"n8n-nodes-base.manualTrigger","typeVersion":1,"position":[144,432]},{"parameters":{"operation":"pdf","options":{}},"id":"15d24750-cb47-4a2f-a6f5-6c2c7c3c39bf","name":"Extract from File","type":"n8n-nodes-base.extractFromFile","typeVersion":1,"position":[720,464]},{"parameters":{"mode":"runOnceForEachItem","jsCode":"const chunks = [];\nconst chunkSize = 1000;\nconst chunkOverlap = 200;\nconst text = $('Extract from File').item.json.text.replace(/\\n/, '');\n\nfor (let i=0,j=Math.round(text.length/chunkSize)+1;i<j;i++) {\n  chunks.push(\n    text.substr(\n      Math.max(0,(i * chunkSize)-chunkOverlap),\n      chunkSize\n    )\n  );\n}\n\nreturn { chunks };"},"id":"7e449613-de9a-49f5-8e84-4728df3b4230","name":"Create Chunks From Doc","type":"n8n-nodes-base.code","typeVersion":2,"position":[1504,544]},{"parameters":{"jsonMode":"expressionData","jsonData":"={{\n{\n  \"content\": `${$json.text }\\n---\\n${$json.chunk}`\n}\n}}","options":{"metadata":{"metadataValues":[{"name":"title","value":"={{ $json.title }}"}]}}},"id":"afe1844c-6e3b-4120-b9ba-fd52203c6287","name":"Default Data Loader","type":"@n8n/n8n-nodes-langchain.documentDefaultDataLoader","typeVersion":1,"position":[3248,752]},{"parameters":{"chunkSize":2000,"options":{}},"id":"e9e549da-e61f-49c3-8657-35a1367e1db8","name":"Recursive Character Text Splitter","type":"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter","typeVersion":1,"position":[3248,864]},{"parameters":{"options":{}},"id":"935a5e6b-455b-43ac-9d15-f0120167629d","name":"When chat message received","type":"@n8n/n8n-nodes-langchain.chatTrigger","typeVersion":1.1,"position":[592,1360],"webhookId":"436ca65c-46ca-4f8c-86e2-b8633b428eea"},{"parameters":{},"id":"95116782-2619-4a34-aafe-84c17e72b8ea","name":"Window Buffer Memory","type":"@n8n/n8n-nodes-langchain.memoryBufferWindow","typeVersion":1.2,"position":[912,1552]},{"parameters":{"content":"## 2. Split Document Into Chunks\nUnlike traditional vector store workflows, we want to split our document prior to embedding and this is easily achieved using the Code node. Feel free to adjust the text splitting params or replace it entirely to suit the needs of your data.\n\nYou may need to play around and adjust the chunksize for your particular data. Contextual retrieval as described in the article is expected to return 20 results so best to keep these small.","height":513.3089035768523,"width":553.1909664515983,"color":7},"id":"51e9975e-3542-4600-91f9-a0f960fed5e8","name":"Sticky Note","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1280,288]},{"parameters":{"content":"## 3. Generate Sparse Vector and Contextual Text For Chunk\nWith our chunks, we'll want to achieve the following:\n(1) **Generate a contextual summary of what the chunk is about relative to the whole document**.\nFor this, we'll use the basic LLM node using Antrophic's Claude Haiku model with the recommended prompt as shared in the article.\n(2) **Generate a sparse vector for the chunk and summary**\nWe can use the python code node to generate TF-IDF sparse vectors with the scikit-learn library. Good to know, this library doesn't require external dependency setup steps and auto-installs on first time use.\n\nOnce we have our generated values, we'll combine them with the chunk object using the Edit Fields node.","height":748.1255853485875,"width":1019.742667955435,"color":7},"id":"297481f4-7a7a-484d-8638-d570b7a218dc","name":"Sticky Note1","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1872,288]},{"parameters":{"content":"## 4. Insert Docs to Qdrant (via Langchain Code Node)\nUnfortunately, n8n (or rather langchain) doesn't support inserting sparse vectors so we'll have to build our own \"Insert Documents\" node using a Langchain Code Node. In this Langchain code node, we'll forego the langchain vectorstore node and use the Qdrant client SDK directly instead.\n\n**Note** To avoid duplication, this node will also delete existing vectors by document title. It does so by tagging each vector with the document title we extracted earlier then when run again, performs a Qdrant delete by filter before upserting.\n\n**Required:**\nTo use this demonstration, you must complete the following:\n* Ensure your Qdrant instance is running and set the URL in the node\n* Create the Qdrant collection as instructed (see yellow sticky)\n","height":783.6896392386983,"width":828.7526122872351,"color":7},"id":"2240a74e-72ef-4040-94d7-641b52b083ac","name":"Sticky Note2","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[2928,256]},{"parameters":{"content":"## 5. Retrieval using Sparse Vectors and ReRanker (Chat Agent Example)\nFor retrieval, we want to be able to (1) query with both dense and sparse vectors and (2) apply a rerank algorithm to our vector store docs. We can setup a custom vector store tool which does both using a custom Langchain Code node.\n\n**Required:**\nTo use this demonstration, you must complete the following:\n* Installed the updated version of the @Qdrant/js-client-rest module\n* Ensure your Qdrant instance is running and set the URL in the \"Qdrant with Cohere ReRank\" subnode\n* Add your Cohere API key in the \"Qdrant with Cohere ReRank\" subnode.","height":828.8619472986827,"width":973.8052093023243,"color":7},"id":"65beabd7-3960-400e-a709-795c0a42944a","name":"Sticky Note3","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[448,1072]},{"parameters":{},"id":"b4f34053-3278-48e5-9695-adbfe72a2d31","name":"Execute Workflow Trigger","type":"n8n-nodes-base.executeWorkflowTrigger","typeVersion":1,"position":[1504,1888]},{"parameters":{"name":"get_sparse_vector","description":"Generates TD-IDF sparse vector for query","workflowId":{"__rl":true,"value":"={{ $workflow.id }}","mode":"id"},"fields":{"values":[{"name":"route","stringValue":"get_sparse_vectors"}]}},"id":"8d9af95d-39bc-4002-a7dc-f25b053e7df4","name":"Get Sparse Vector Tool","type":"@n8n/n8n-nodes-langchain.toolWorkflow","typeVersion":1.2,"position":[1184,1680]},{"parameters":{"assignments":{"assignments":[{"id":"87bc3071-4179-4aed-aa88-37c6381d8b73","name":"query","value":"Who created Bitcoin?","type":"string"}]},"options":{}},"id":"a7c8141e-4d86-4874-bdea-f4682d3310ca","name":"Query","type":"n8n-nodes-base.set","typeVersion":3.4,"position":[1632,1344]},{"parameters":{"name":"get_sparse_vector","description":"Generates TD-IDF sparse vector for query","workflowId":{"__rl":true,"value":"={{ $workflow.id }}","mode":"id"}},"id":"f7ab6882-0f0a-455d-ba2b-c5656797f2c9","name":"Get Sparse Vector Tool1","type":"@n8n/n8n-nodes-langchain.toolWorkflow","typeVersion":1.2,"position":[2000,1520]},{"parameters":{"content":"## 6. Retrieval using Sparse Vectors and ReRanker (Retrieval Example)\nThis demonstration is similar to the previous step but is not using an AI Agent.\n\n**Required:**\nTo use this demonstration, you must complete the following:\n* Installed the updated version of the @Qdrant/js-client-rest module\n* Ensure your Qdrant instance is running and set the URL in the \"Qdrant with Cohere ReRank1\" node\n* Add your Cohere API key in the \"Qdrant with Cohere ReRank1\" node.","height":683.3722976015338,"width":838.4124151865863,"color":7},"id":"e50993ce-3b18-4d13-95bf-45f3b93011a1","name":"Sticky Note4","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1440,1072]},{"parameters":{"content":"### Create Collection!\nYou need to create a Qdrant Collection as follows:\n\n* Go to http[s]:\\//<qdrant_url>/dashboard#/console\nIf you are hosting locally, this is usually http://localhost:6333/dashboard#/console\n* Copy the following into the left panel. This will tell Qdrant to create a new collection called “contextual_retrieval_example”. You can change this of course but you’ll also need to change all “collectionName” references in the template as well!\n\n```\nPUT collections/contextual_retrieval_example\n{\n  \"vectors\": {\n    \"default\": {\n      \"distance\": \"Cosine\",\n      \"size\": 1024\n    }\n  },\n  \"sparse_vectors\": {\n    \"bm42\": {\n      \"modifier\": \"idf\"\n    }\n  }\n}\n```","height":505.701259707935,"width":516.3129732020735},"id":"d32045c3-ec76-404e-b72f-c5914fcfac30","name":"Sticky Note5","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[3648,592]},{"parameters":{"fieldToSplitOut":"chunks","options":{"destinationFieldName":"chunk"}},"id":"b5bdd033-6c07-4c78-8478-bb4314924aef","name":"Chunks To List","type":"n8n-nodes-base.splitOut","typeVersion":1,"position":[1984,608]},{"parameters":{"mode":"runOnceForEachItem","language":"python","pythonCode":"texts = [f\"{_('Generate Contextual Text').item.json.text}\\n---\\n{_('Chunks To List').item.json.chunk}\"]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the texts to generate TF-IDF vectors\nX = vectorizer.fit_transform(texts)\n\nreturn {\n  \"sparse\": {\n    \"indices\": X.indices.tolist(),\n    \"values\": X.data.tolist()\n  }\n}"},"id":"efc70be7-f2c4-42f5-beb4-07a8780ca133","name":"Generate TF-IDF Sparse Vectors","type":"n8n-nodes-base.code","typeVersion":2,"position":[2512,608]},{"parameters":{"assignments":{"assignments":[{"id":"069d067c-3534-4939-8ff4-34dee02a9436","name":"chunk","value":"={{ $('Chunks To List').item.json.chunk }}","type":"string"},{"id":"24e01f4f-e156-47e9-a89e-9cbdccda6bd4","name":"text","value":"={{ $('Generate Contextual Text').item.json.text }}","type":"string"},{"id":"fa48ddaa-4658-463a-b1af-8308c24e325a","name":"sparse","value":"={{ $json.sparse }}","type":"object"},{"id":"442efe87-a826-402c-aadc-909923915d30","name":"title","value":"={{ $('Get Doc Attributes').first().json.output.title }}","type":"string"}]},"options":{}},"id":"3d1552e2-10d3-4be8-8711-cec65f49a39a","name":"Get Values","type":"n8n-nodes-base.set","typeVersion":3.4,"position":[2688,608]},{"parameters":{"mode":"runOnceForEachItem","language":"python","pythonCode":"import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the texts to generate TF-IDF vectors\ntexts = [_input.item.json.query]\nX = vectorizer.fit_transform(texts)\n\nreturn {\n  \"response\": {\n    \"indices\": X.indices.tolist(),\n    \"values\": X.data.tolist()\n  }\n}"},"id":"3e475f0a-f290-48bc-aee8-9b2e3e8f719f","name":"Generate Sparse Vectors","type":"n8n-nodes-base.code","typeVersion":2,"position":[1904,1888]},{"parameters":{"content":"## Implementing Antrophic's Contextual Retrieval \n\nThis workflow is inspired by the Antrophic article, [\"Introducing Contextual Retrieval\"](https://www.anthropic.com/news/contextual-retrieval), and tries to replicate the instructions given.\n\n**Original post here**: https://community.n8n.io/t/building-the-ultimate-rag-setup-with-contextual-summaries-sparse-vectors-and-reranking/54861/1\n\n### Changelog\n2024-10-10 Updated to 1.62.1\n\n### Need Help?\nJoin the [Discord](https://discord.com/invite/XPKeKXeB7d) or ask in the [Forum](https://community.n8n.io/)!\n","height":634.9891793542789,"width":405.66570496276904},"id":"52089247-4fab-428a-88c8-a7f2e13b50ed","name":"Sticky Note6","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[0,0]},{"parameters":{"content":"### PART 1 of 2\nThis part generates and inserts into the vector store. You only have to do this once per document.","height":102.577757187954,"width":389.2978313113204,"color":5},"id":"6c4f2d15-2817-4466-9e19-928e664998f4","name":"Sticky Note8","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[448,160]},{"parameters":{"content":"### PART 2 of 2\nThis part demostrates 2 examples of retrieve or query your sparse vectors.","height":80,"width":524.5591143796955,"color":5},"id":"6c572246-a09a-4a4c-9eec-dd204870697c","name":"Sticky Note9","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[448,960]},{"parameters":{"rules":{"values":[{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"strict","version":2},"conditions":[{"leftValue":"={{ $json.route }}","rightValue":"get_sparse_vectors","operator":{"type":"string","operation":"equals"}}],"combinator":"and"},"renameOutput":true,"outputKey":"get sparse vectors"},{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"strict","version":2},"conditions":[{"id":"84ac9b84-0e46-45da-b719-843d947ea429","leftValue":"={{ $json.route }}","rightValue":"my_other_thing","operator":{"type":"string","operation":"equals","name":"filter.operator.equals"}}],"combinator":"and"},"renameOutput":true,"outputKey":"my other thing"}]},"options":{"fallbackOutput":"none"}},"id":"abf4bc35-8f09-4862-bd90-feafe9d7e79b","name":"Router","type":"n8n-nodes-base.switch","typeVersion":3.2,"position":[1664,1888]},{"parameters":{"promptType":"define","text":"=<document> \n{{ $('Extract from File').first().json.text }} \n</document>\nHere is the chunk we want to situate within the whole document \n<chunk> \n{{ $json.chunk }}\n</chunk> \nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. "},"id":"d3c2ed0e-58c2-418c-8615-a7acd639da6c","name":"Generate Contextual Text","type":"@n8n/n8n-nodes-langchain.chainLlm","typeVersion":1.4,"position":[2160,608]},{"parameters":{"options":{}},"id":"eed71817-664c-4e29-bb92-6577474af9b0","name":"AI Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":1.6,"position":[784,1360]},{"parameters":{"content":"### Sparse Vector Tool for Agent\nUnfortunately there is a bug linking code tool to custom langchain code node so this is the only approach until that is fixed!","height":287.1680736478712,"width":652.0156501726113,"color":6},"id":"6ce0a1bf-0565-4413-a02e-b7070628e89f","name":"Sticky Note7","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[1456,1776]},{"parameters":{"url":"https://bitcoin.org/bitcoin.pdf","options":{}},"id":"745cc1aa-1cb7-47bb-a223-e687102d1a31","name":"Get Document","type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[544,464]},{"parameters":{"text":"={{ $json.text }}","attributes":{"attributes":[{"name":"title","description":"The title of the document.","required":true}]},"options":{}},"id":"3a2ffd45-1569-41e8-95c4-0070798d897c","name":"Get Doc Attributes","type":"@n8n/n8n-nodes-langchain.informationExtractor","typeVersion":1,"position":[912,464]},{"parameters":{"content":"## 1. Import Document PDF\n\nFor this demonstration, we'll use the Bitcoin Whitepaper from bitcoin.org.\nWe'll take this time to also extract specific document attributes which we can use as metadata.","height":513.3089035768523,"width":807.2147979360316,"color":7},"id":"f32bfe1b-8178-4684-8ef4-c4997dbb3702","name":"Sticky Note10","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[448,288]},{"parameters":{"model":"llama3.2:latest","options":{}},"id":"434d5198-01bb-4cc4-9ad1-41e7fe349fb4","name":"Ollama Chat Model","type":"@n8n/n8n-nodes-langchain.lmChatOllama","typeVersion":1,"position":[912,640]},{"parameters":{"model":"llama3.2:latest","options":{}},"id":"7b522795-95ed-447c-82f4-0ffe0502a4aa","name":"Ollama Chat Model1","type":"@n8n/n8n-nodes-langchain.lmChatOllama","typeVersion":1,"position":[2160,832]},{"parameters":{"model":"nomic-embed-text:latest"},"id":"e2a386bf-9950-420c-9f2a-a73774641ca4","name":"Embeddings Ollama","type":"@n8n/n8n-nodes-langchain.embeddingsOllama","typeVersion":1,"position":[3104,752]},{"parameters":{"model":"llama3.2:latest"},"id":"5dea8c0c-8446-4255-b1c2-cb316e9e668c","name":"Embeddings Ollama1","type":"@n8n/n8n-nodes-langchain.embeddingsOllama","typeVersion":1,"position":[1024,1680]},{"parameters":{"model":"nomic-embed-text:latest"},"id":"fed2beaf-9c8a-4752-8fcf-d10e1d7c7c12","name":"Embeddings Ollama2","type":"@n8n/n8n-nodes-langchain.embeddingsOllama","typeVersion":1,"position":[1808,1520]},{"parameters":{"model":"llama3.2:latest","options":{}},"id":"bb2d7e93-1fa8-4764-9dae-33878e0516eb","name":"Ollama Chat Model2","type":"@n8n/n8n-nodes-langchain.lmChatOllama","typeVersion":1,"position":[784,1552]},{"parameters":{"content":"### NO SUPPORT PROVIDED\nUnfortunately, local models are still currently prone to a lot of stability and accurracy issues which are near impossible for me to reproduce or explain. If you use this template, you do so understanding that I can't help with fixing your issues and you just have to figure it out for yourself.","height":140.02059752369627,"width":509.7219334301004,"color":3},"id":"b176b6f9-400f-40c7-8a18-049400297583","name":"Sticky Note11","type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[448,0]},{"parameters":{"code":{"supplyData":{"code":"const { QdrantClient } = require('@qdrant/js-client-rest');\nconst { BM25Retriever } = require(\"@langchain/community/retrievers/bm25\");\nconst { DynamicTool } = require(\"@langchain/core/tools\");\n\n// 1. Tool Config\nconst name = 'bitcoin_whitepaper_tool';\nconst description = 'Call this tool to get information and/or context from the Bitcoin Whitepaper';\n\n// 2. Qdrant config\nconst client = new QdrantClient({ url: 'http://qdrant:6333' });\nconst collectionName = 'contextual_retrieval_example';\nconst limit = 20;\n\n// 3. Inputs\nconst embeddings = await this.getInputConnectionData('ai_embedding', 0);\nconst sparseVectorTool = await this.getInputConnectionData('ai_tool', 0);\n\n// 4. Tool definition\nconst vectorStoreTool = new DynamicTool({\n  name,\n  description,\n  func: async (input) => {\n    const denseVector = await embeddings.embedQuery(input);\n    const sparseVector = JSON.parse(await sparseVectorTool.invoke(input));\n\n    const response = await client.query(collectionName, {\n      prefetch: [\n        {\n          query: denseVector,\n          using: 'default',\n          limit: 100\n        },\n        {\n          query: sparseVector,\n          using: 'bm42',\n          limit: 100\n        }\n     ],\n     query: { fusion: 'rrf' },\n     with_payload: true,\n     limit,\n    });\n    \n    const docs = response.points.map(res => ({\n      pageContent: res.payload.content,\n      metadata: res.payload.metadata\n    }));\n    const retriever = BM25Retriever.fromDocuments(docs, { k: limit });\n    const rankedDocs = await retriever.invoke(query);\n    return JSON.stringify(rankedDocs);\n  }\n});\n\nreturn vectorStoreTool;"}},"inputs":{"input":[{"type":"ai_embedding","maxConnections":1,"required":true},{"type":"ai_tool","maxConnections":1,"required":true}]},"outputs":{"output":[{"type":"ai_tool"}]}},"id":"f90ddd9d-e59e-489c-9f38-f027685d79a1","name":"Qdrant with BM25 ReRank","type":"@n8n/n8n-nodes-langchain.code","typeVersion":1,"position":[1024,1552]},{"parameters":{"code":{"execute":{"code":"const { QdrantClient } = require('@qdrant/js-client-rest');\nconst { BM25Retriever } = require(\"@langchain/community/retrievers/bm25\");\nconst { DynamicTool } = require(\"@langchain/core/tools\");\n\n// 1. Tool Config\nconst name = 'bitcoin_whitepaper_tool';\nconst description = 'Call this tool to get information and/or context from the Bitcoin Whitepaper';\n\n// 2. Qdrant config\nconst client = new QdrantClient({ url: 'http://qdrant:6333' });\nconst collectionName = 'contextual_retrieval_example';\nconst limit = 20;\n\n// 3. Inputs\nconst inputData = await this.getInputData();\nconst embeddings = await this.getInputConnectionData('ai_embedding', 0);\nconst sparseVectorTool = await this.getInputConnectionData('ai_tool', 0);\n\n// 4. Execute\nconst query = inputData[0].json.query;\n\nconst denseVector = await embeddings.embedQuery(query);\nconst sparseVector = JSON.parse(await sparseVectorTool.invoke(query));\n\nconst response = await client.query(collectionName, {\n  prefetch: [\n    {\n      query: denseVector,\n      using: 'default',\n      limit: 100\n    },\n    {\n      query: sparseVector,\n      using: 'bm42',\n      limit: 100\n    }\n ],\n query: { fusion: 'rrf' },\n with_payload: true,\n limit,\n});\n\nconst docs = response.points.map(res => ({\n  pageContent: res.payload.content,\n  metadata: res.payload.metadata\n}));\nconst retriever = BM25Retriever.fromDocuments(docs, { k: limit });\nconst rankedDocs = await retriever.invoke(query);\nreturn rankedDocs;"}},"inputs":{"input":[{"type":"main","maxConnections":1,"required":true},{"type":"ai_embedding","maxConnections":1,"required":true},{"type":"ai_tool","maxConnections":1,"required":true}]},"outputs":{"output":[{"type":"main"}]}},"id":"6b2f297e-3b5f-4d29-9254-3fde5b3f2a4e","name":"Qdrant with BM25 ReRank1","type":"@n8n/n8n-nodes-langchain.code","typeVersion":1,"position":[1808,1344]},{"parameters":{"code":{"execute":{"code":"const { randomUUID } = require('crypto') // Enable the crypto lib using env var NODE_FUNCTION_ALLOW_BUILTIN=crypto\nconst { QdrantClient } = require('@qdrant/js-client-rest');\n\n// 1. Qdrant config\nconst client = new QdrantClient({ url: 'http://qdrant:6333' });\nconst collectionName = 'contextual_retrieval_example';\n\n// 2. Inputs\nconst inputData = this.getInputData();\nconst embeddings = await this.getInputConnectionData('ai_embedding', 0);\nconst documentLoader = await this.getInputConnectionData('ai_document', 0);\n\n// 3. Run document loader\nconst docs = await documentLoader.processAll(inputData);\n\n// 4. generate points with sparse vectors\nconst points = [];\nlet vector = {};\nfor (let i=0,j=docs.length;i<j;i++) {\n  points.push({\n    id: randomUUID(),\n    vector: {\n      default: await embeddings.embedQuery(docs[i].pageContent),\n      bm42: inputData[i].json.sparse,\n    },\n    payload: {\n      content: docs[i].pageContent,\n      metadata: docs[i].metadata,\n    }\n  })\n}\n\n// 5. Delete existing vectors by title\nawait client.delete(collectionName, {\n  filter: {\n    must: [\n      {\n        key: \"metadata.title\",\n        match: { \"value\": docs[0].metadata.title }\n      }\n    ]\n  }\n});\n\n// 6. Upsert into Qdrant\nconst res = await client.upsert(collectionName, { points });\n\nreturn res;"}},"inputs":{"input":[{"type":"main","maxConnections":1,"required":true},{"type":"ai_embedding","maxConnections":1,"required":true},{"type":"ai_document","maxConnections":1,"required":true}]},"outputs":{"output":[{"type":"main"}]}},"id":"b26be739-878b-40ca-bdb8-93077345f015","name":"Insert Documents with Sparse Vectors","type":"@n8n/n8n-nodes-langchain.code","typeVersion":1,"position":[3152,560]}],"connections":{"When clicking ‘Test workflow’":{"main":[[{"node":"Get Document","type":"main","index":0}]]},"Extract from File":{"main":[[{"node":"Get Doc Attributes","type":"main","index":0}]]},"Create Chunks From Doc":{"main":[[{"node":"Chunks To List","type":"main","index":0}]]},"Default Data Loader":{"ai_document":[[{"node":"Insert Documents with Sparse Vectors","type":"ai_document","index":0}]]},"Recursive Character Text Splitter":{"ai_textSplitter":[[{"node":"Default Data Loader","type":"ai_textSplitter","index":0}]]},"When chat message received":{"main":[[{"node":"AI Agent","type":"main","index":0}]]},"Window Buffer Memory":{"ai_memory":[[{"node":"AI Agent","type":"ai_memory","index":0}]]},"Execute Workflow Trigger":{"main":[[{"node":"Router","type":"main","index":0}]]},"Get Sparse Vector Tool":{"ai_tool":[[{"node":"Qdrant with BM25 ReRank","type":"ai_tool","index":0}]]},"Query":{"main":[[{"node":"Qdrant with BM25 ReRank1","type":"main","index":0}]]},"Get Sparse Vector Tool1":{"ai_tool":[[{"node":"Qdrant with BM25 ReRank1","type":"ai_tool","index":0}]]},"Chunks To List":{"main":[[{"node":"Generate Contextual Text","type":"main","index":0}]]},"Generate TF-IDF Sparse Vectors":{"main":[[{"node":"Get Values","type":"main","index":0}]]},"Get Values":{"main":[[{"node":"Insert Documents with Sparse Vectors","type":"main","index":0}]]},"Router":{"main":[[{"node":"Generate Sparse Vectors","type":"main","index":0}]]},"Generate Contextual Text":{"main":[[{"node":"Generate TF-IDF Sparse Vectors","type":"main","index":0}]]},"Get Document":{"main":[[{"node":"Extract from File","type":"main","index":0}]]},"Get Doc Attributes":{"main":[[{"node":"Create Chunks From Doc","type":"main","index":0}]]},"Ollama Chat Model":{"ai_languageModel":[[{"node":"Get Doc Attributes","type":"ai_languageModel","index":0}]]},"Ollama Chat Model1":{"ai_languageModel":[[{"node":"Generate Contextual Text","type":"ai_languageModel","index":0}]]},"Embeddings Ollama":{"ai_embedding":[[{"node":"Insert Documents with Sparse Vectors","type":"ai_embedding","index":0}]]},"Embeddings Ollama1":{"ai_embedding":[[{"node":"Qdrant with BM25 ReRank","type":"ai_embedding","index":0}]]},"Embeddings Ollama2":{"ai_embedding":[[{"node":"Qdrant with BM25 ReRank1","type":"ai_embedding","index":0}]]},"Ollama Chat Model2":{"ai_languageModel":[[{"node":"AI Agent","type":"ai_languageModel","index":0}]]},"Qdrant with BM25 ReRank":{"ai_tool":[[{"node":"AI Agent","type":"ai_tool","index":0}]]}},"settings":{"executionOrder":"v1"},"staticData":null,"meta":null,"pinData":{},"versionId":"972b3609-cdb7-4b06-b94a-9c06b1eaa4fd","triggerCount":0,"tags":[]}